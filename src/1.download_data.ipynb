{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Data\n",
    "Saves non-video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T20:18:36.735568Z",
     "start_time": "2020-11-27T20:18:36.509643Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T00:35:16.739189Z",
     "start_time": "2020-12-03T00:35:16.661274Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import re\n",
    "import google_auth_oauthlib.flow\n",
    "from googleapiclient.discovery import build\n",
    "import googleapiclient.errors\n",
    "from pprint import pprint\n",
    "import json\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from tqdm import tqdm\n",
    "import youtube_dl\n",
    "import pickle\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from lib.Episode import Episode, EpisodeFactory\n",
    "import time\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T00:36:36.082281Z",
     "start_time": "2020-12-03T00:36:36.072903Z"
    }
   },
   "outputs": [],
   "source": [
    "ID = \"lex\"\n",
    "BASE = f\"../data/{ID}\"\n",
    "VIDEOS_LOCATION = f\"/Volumes/{ID}/videos/\"\n",
    "WEBSITE = f\"../../{ID}-vis/public/\"\n",
    "\n",
    "UPLOADS = BASE + \"/uploads.json\"\n",
    "STATS = BASE + \"/stats.json\"\n",
    "COMMENTS = BASE + \"/comments.json\"\n",
    "CAPTIONS = BASE + \"/captions.json\"\n",
    "CAPTIONS_FAILED = BASE + \"/captions_failure.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T00:36:56.490084Z",
     "start_time": "2020-12-03T00:36:56.160610Z"
    }
   },
   "outputs": [],
   "source": [
    "# Disable OAuthlib's HTTPS verification when running locally.\n",
    "# *DO NOT* leave this option enabled in production.\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n",
    "API_KEY = \"AIzaSyApvC_9XhjS4Hun7JqqAsUuAL3eXhY8rmM\"\n",
    "\n",
    "os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "\n",
    "# https://developers.google.com/resources/api-libraries/documentation/youtube/v3/python/latest/\n",
    "youtube = build(api_service_name, api_version, developerKey=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T00:37:37.706802Z",
     "start_time": "2020-12-03T00:37:37.566972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kind': 'youtube#channelListResponse',\n",
       " 'etag': 'aY9fEZBveDyoom-c6zy897RtVYA',\n",
       " 'pageInfo': {'totalResults': 1, 'resultsPerPage': 5},\n",
       " 'items': [{'kind': 'youtube#channel',\n",
       "   'etag': '00S2G4SJGIaEORsQ1zHp-OEyjB4',\n",
       "   'id': 'UCSHZKyawb77ixDdsGog4iWA',\n",
       "   'contentDetails': {'relatedPlaylists': {'likes': '',\n",
       "     'favorites': '',\n",
       "     'uploads': 'UUSHZKyawb77ixDdsGog4iWA'}}}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube.channels().list(\n",
    "    # part=\"snippet,contentDetails,statistics,uploads\",\n",
    "    part=\"contentDetails\",\n",
    "    forUsername=\"lexfridman\"\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T00:39:06.727137Z",
     "start_time": "2020-12-03T00:39:06.723160Z"
    }
   },
   "outputs": [],
   "source": [
    "UPLOADS_ID = \"UUzQUP1qoWDoEbmsQxvdjxgQ\" # JRE\n",
    "UPLOADS_ID = \"UUSHZKyawb77ixDdsGog4iWA\" # LEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T00:39:17.186769Z",
     "start_time": "2020-12-03T00:39:15.774170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/509 -> CPQDEAA\r"
     ]
    }
   ],
   "source": [
    "### Get uploads\n",
    "def get_video_ids(upload_id):\n",
    "    page_token = None\n",
    "    items = []\n",
    "    total_results = 0\n",
    "\n",
    "    while True:\n",
    "        print(f\"{len(items)}/{total_results} -> {page_token}\", end=\"\\r\")\n",
    "\n",
    "        request = youtube.playlistItems().list(\n",
    "            part=\"id,contentDetails\",\n",
    "            playlistId=upload_id,\n",
    "            pageToken=page_token,\n",
    "            maxResults=50,\n",
    "        )\n",
    "        response = request.execute()\n",
    "        items.extend([item[\"contentDetails\"][\"videoId\"] for item in response[\"items\"]])\n",
    "\n",
    "        total_results = response[\"pageInfo\"][\"totalResults\"]\n",
    "\n",
    "        if \"nextPageToken\" in response:\n",
    "            page_token = response[\"nextPageToken\"]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return items\n",
    "\n",
    "try:\n",
    "    video_ids = get_video_ids(UPLOADS_ID)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T00:39:18.074230Z",
     "start_time": "2020-12-03T00:39:18.052077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "509"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T00:40:53.699622Z",
     "start_time": "2020-12-03T00:39:39.100148Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 509/509 [01:14<00:00,  6.84it/s]\n"
     ]
    }
   ],
   "source": [
    "def get_uploads():\n",
    "    uploads = []\n",
    "    \n",
    "    for id in tqdm(video_ids): \n",
    "        response = youtube.videos().list(\n",
    "            id=id,\n",
    "            part=\"contentDetails,id,liveStreamingDetails,recordingDetails,snippet,statistics,topicDetails,status\",\n",
    "        ).execute()\n",
    "        \n",
    "        # Sanity check to ensure all responses have length 1\n",
    "        if len(response['items']) != 1:\n",
    "            print(\"There should not be more than one item for\", id)\n",
    "        \n",
    "        uploads.append(response['items'][0])\n",
    "        \n",
    "    return uploads\n",
    "\n",
    "uploads = get_uploads()\n",
    "with open(UPLOADS, \"w\") as f:\n",
    "    f.write(json.dumps(uploads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-03T00:40:53.737534Z",
     "start_time": "2020-12-03T00:40:53.702075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(509,\n",
       " {'kind': 'youtube#video',\n",
       "  'etag': '_iMQxh8EnTTt_V86KIUqiYp3erk',\n",
       "  'id': '_L3gNaAVjQ4',\n",
       "  'snippet': {'publishedAt': '2020-10-22T01:09:25Z',\n",
       "   'channelId': 'UCSHZKyawb77ixDdsGog4iWA',\n",
       "   'title': 'George Hotz: Hacking the Simulation & Learning to Drive with Neural Nets | Lex Fridman Podcast #132',\n",
       "   'description': \"George Hotz (geohot) is a programmer, hacker, and the founder of Comma.ai. Please support this podcast by checking out our sponsors:\\n- Four Sigmatic: https://foursigmatic.com/lex and use code LexPod to get up to 40% & free shipping\\n- Decoding Digital: https://appdirect.com/decoding-digital\\n- ExpressVPN: https://expressvpn.com/lexpod and use code LexPod to get 3 months free\\n\\nEPISODE LINKS:\\nComma.ai's Twitter: https://twitter.com/comma_ai\\nComma.ai's Website: https://comma.ai/\\nGeorge's Instagram: https://www.instagram.com/georgehotz\\nGeorge's Twitch: https://www.twitch.tv/georgehotz\\nGeorge's Twitter: https://twitter.com/realgeorgehotz\\nComma.ai YouTube (unofficial): https://www.youtube.com/channel/UCwgKmJM4ZJQRJ-U5NjvR2dg\\n\\nPODCAST INFO:\\nPodcast website: https://lexfridman.com/podcast\\nApple Podcasts: https://apple.co/2lwqZIr\\nSpotify: https://spoti.fi/2nEwCF8\\nRSS: https://lexfridman.com/feed/podcast/\\nFull episodes playlist: https://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4\\nClips playlist: https://www.youtube.com/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41\\n\\nOUTLINE:\\n0:00 - Introduction\\n2:31 - Will human civilization destroy itself?\\n5:18 - Where are the aliens?\\n10:05 - Tic Tac UFO and Bob Lazar\\n12:33 - Conspiracy theories\\n14:36 - The programming language of life\\n18:57 - The games that humans play\\n27:27 - Memory leaks in the simulation\\n29:58 - Theories of everything\\n31:43 - Ethereum startup story\\n39:30 - Cryptocurrency\\n48:57 - Self-help advice\\n52:37 - Comma.ai\\n54:30 - Comma two\\n1:03:19 - Tesla vs Comma.ai\\n1:12:22 - Driver monitoring\\n1:26:03 - Communicating uncertainty\\n1:27:51 - Tesla Dojo\\n1:34:19 - Tesla Autopilot big rewrite\\n1:40:37 - How to install the Comma Two\\n1:45:13 - Openpilot is Android & Autopilot is iOS\\n1:54:28 - Waymo\\n2:05:41 - Autonomous driving and society\\n2:07:53 - Moving\\n2:10:58 - Advice to Startups\\n2:24:00 - Programming setup\\n2:27:01 - Ideas that changed my life\\n2:35:06 - GPT-3\\n2:38:26 - AGI\\n2:42:29 - Programming languages that everyone should learn\\n2:49:02 - How to learn anything\\n2:51:34 - Book recommendations\\n2:59:57 - Love\\n3:01:46 - Psychedelics\\n3:04:07 - Crazy\\n\\nCONNECT:\\n- Subscribe to this YouTube channel\\n- Twitter: https://twitter.com/lexfridman\\n- LinkedIn: https://www.linkedin.com/in/lexfridman\\n- Facebook: https://www.facebook.com/LexFridmanPage\\n- Instagram: https://www.instagram.com/lexfridman\\n- Medium: https://medium.com/@lexfridman\\n- Support on Patreon: https://www.patreon.com/lexfridman\",\n",
       "   'thumbnails': {'default': {'url': 'https://i.ytimg.com/vi/_L3gNaAVjQ4/default.jpg',\n",
       "     'width': 120,\n",
       "     'height': 90},\n",
       "    'medium': {'url': 'https://i.ytimg.com/vi/_L3gNaAVjQ4/mqdefault.jpg',\n",
       "     'width': 320,\n",
       "     'height': 180},\n",
       "    'high': {'url': 'https://i.ytimg.com/vi/_L3gNaAVjQ4/hqdefault.jpg',\n",
       "     'width': 480,\n",
       "     'height': 360},\n",
       "    'standard': {'url': 'https://i.ytimg.com/vi/_L3gNaAVjQ4/sddefault.jpg',\n",
       "     'width': 640,\n",
       "     'height': 480},\n",
       "    'maxres': {'url': 'https://i.ytimg.com/vi/_L3gNaAVjQ4/maxresdefault.jpg',\n",
       "     'width': 1280,\n",
       "     'height': 720}},\n",
       "   'channelTitle': 'Lex Fridman',\n",
       "   'tags': ['george hotz',\n",
       "    'artificial intelligence',\n",
       "    'agi',\n",
       "    'ai',\n",
       "    'ai podcast',\n",
       "    'artificial intelligence podcast',\n",
       "    'lex fridman',\n",
       "    'lex podcast',\n",
       "    'lex mit',\n",
       "    'lex ai',\n",
       "    'lex jre',\n",
       "    'mit ai'],\n",
       "   'categoryId': '28',\n",
       "   'liveBroadcastContent': 'none',\n",
       "   'defaultLanguage': 'en',\n",
       "   'localized': {'title': 'George Hotz: Hacking the Simulation & Learning to Drive with Neural Nets | Lex Fridman Podcast #132',\n",
       "    'description': \"George Hotz (geohot) is a programmer, hacker, and the founder of Comma.ai. Please support this podcast by checking out our sponsors:\\n- Four Sigmatic: https://foursigmatic.com/lex and use code LexPod to get up to 40% & free shipping\\n- Decoding Digital: https://appdirect.com/decoding-digital\\n- ExpressVPN: https://expressvpn.com/lexpod and use code LexPod to get 3 months free\\n\\nEPISODE LINKS:\\nComma.ai's Twitter: https://twitter.com/comma_ai\\nComma.ai's Website: https://comma.ai/\\nGeorge's Instagram: https://www.instagram.com/georgehotz\\nGeorge's Twitch: https://www.twitch.tv/georgehotz\\nGeorge's Twitter: https://twitter.com/realgeorgehotz\\nComma.ai YouTube (unofficial): https://www.youtube.com/channel/UCwgKmJM4ZJQRJ-U5NjvR2dg\\n\\nPODCAST INFO:\\nPodcast website: https://lexfridman.com/podcast\\nApple Podcasts: https://apple.co/2lwqZIr\\nSpotify: https://spoti.fi/2nEwCF8\\nRSS: https://lexfridman.com/feed/podcast/\\nFull episodes playlist: https://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4\\nClips playlist: https://www.youtube.com/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41\\n\\nOUTLINE:\\n0:00 - Introduction\\n2:31 - Will human civilization destroy itself?\\n5:18 - Where are the aliens?\\n10:05 - Tic Tac UFO and Bob Lazar\\n12:33 - Conspiracy theories\\n14:36 - The programming language of life\\n18:57 - The games that humans play\\n27:27 - Memory leaks in the simulation\\n29:58 - Theories of everything\\n31:43 - Ethereum startup story\\n39:30 - Cryptocurrency\\n48:57 - Self-help advice\\n52:37 - Comma.ai\\n54:30 - Comma two\\n1:03:19 - Tesla vs Comma.ai\\n1:12:22 - Driver monitoring\\n1:26:03 - Communicating uncertainty\\n1:27:51 - Tesla Dojo\\n1:34:19 - Tesla Autopilot big rewrite\\n1:40:37 - How to install the Comma Two\\n1:45:13 - Openpilot is Android & Autopilot is iOS\\n1:54:28 - Waymo\\n2:05:41 - Autonomous driving and society\\n2:07:53 - Moving\\n2:10:58 - Advice to Startups\\n2:24:00 - Programming setup\\n2:27:01 - Ideas that changed my life\\n2:35:06 - GPT-3\\n2:38:26 - AGI\\n2:42:29 - Programming languages that everyone should learn\\n2:49:02 - How to learn anything\\n2:51:34 - Book recommendations\\n2:59:57 - Love\\n3:01:46 - Psychedelics\\n3:04:07 - Crazy\\n\\nCONNECT:\\n- Subscribe to this YouTube channel\\n- Twitter: https://twitter.com/lexfridman\\n- LinkedIn: https://www.linkedin.com/in/lexfridman\\n- Facebook: https://www.facebook.com/LexFridmanPage\\n- Instagram: https://www.instagram.com/lexfridman\\n- Medium: https://medium.com/@lexfridman\\n- Support on Patreon: https://www.patreon.com/lexfridman\"},\n",
       "   'defaultAudioLanguage': 'en-US'},\n",
       "  'contentDetails': {'duration': 'PT3H8M46S',\n",
       "   'dimension': '2d',\n",
       "   'definition': 'hd',\n",
       "   'caption': 'false',\n",
       "   'licensedContent': True,\n",
       "   'contentRating': {},\n",
       "   'projection': 'rectangular'},\n",
       "  'status': {'uploadStatus': 'processed',\n",
       "   'privacyStatus': 'public',\n",
       "   'license': 'youtube',\n",
       "   'embeddable': True,\n",
       "   'publicStatsViewable': False,\n",
       "   'madeForKids': False},\n",
       "  'statistics': {'viewCount': '340094',\n",
       "   'likeCount': '11577',\n",
       "   'dislikeCount': '311',\n",
       "   'favoriteCount': '0',\n",
       "   'commentCount': '1896'},\n",
       "  'recordingDetails': {}})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(UPLOADS, \"r\") as f:\n",
    "    uploads = json.load(f)\n",
    "# Contents of uploads\n",
    "len(uploads), uploads[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-03T00:42:45.681Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 38/509 [00:26<06:53,  1.14it/s]"
     ]
    }
   ],
   "source": [
    "def get_captions():\n",
    "    failed_to_get_captions = []\n",
    "    captions = {}\n",
    "    with open(CAPTIONS, \"r\") as f:\n",
    "        captions = json.loads(f.read())\n",
    "    with open(UPLOADS, \"r\") as f:\n",
    "        uploads = json.loads(f.read())\n",
    "    \n",
    "    downloaded = 0\n",
    "    for upload in tqdm(uploads):\n",
    "        id = upload[\"id\"]\n",
    "\n",
    "        if id in captions:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            caption = YouTubeTranscriptApi.get_transcript(id)\n",
    "            captions[id] = caption\n",
    "            downloaded += 1\n",
    "        except:\n",
    "            failed_to_get_captions.append(id)\n",
    "\n",
    "        # save progress\n",
    "        if downloaded % 10 == 0:\n",
    "            with open(CAPTIONS, \"w\") as f:\n",
    "                f.write(json.dumps(captions))\n",
    "\n",
    "    print(\"Downloaded\", downloaded, \"items\")\n",
    "    return captions, failed_to_get_captions\n",
    "\n",
    "captions, failed_to_get_captions = get_captions()\n",
    "\n",
    "with open(CAPTIONS, \"w\") as f:\n",
    "    f.write(json.dumps(captions))\n",
    "\n",
    "with open(CAPTIONS_FAILED, \"w\") as f:\n",
    "    f.write(json.dumps(failed_to_get_captions))\n",
    "    \n",
    "print(len(captions.items()), len(failed_to_get_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-03T00:42:54.479Z"
    }
   },
   "outputs": [],
   "source": [
    "print(len(captions.items()), len(failed_to_get_captions))\n",
    "\n",
    "print(\"Missing the following items\")\n",
    "[[(u['snippet']['title'], u['contentDetails']['duration']) for u in uploads if u['id'] == id][0] for id in failed_to_get_captions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Episodes cache\n",
    "Create episode object (lib/Episode) out of each upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-12-03T00:43:05.928Z"
    }
   },
   "outputs": [],
   "source": [
    "CACHE_ALL = f\"./{ID}-episodes.pickle\"\n",
    "CACHE_SMALL = f\"./{ID}-episodes-small.pickle\"\n",
    "CACHE = CACHE_ALL\n",
    "\n",
    "print(f\"Generating new pickle {CACHE}...\")\n",
    "\n",
    "factory = EpisodeFactory(BASE)\n",
    "episodes = factory.create_episodes(skip_comments=True)\n",
    "\n",
    "with open(CACHE, \"wb\") as f:\n",
    "    if CACHE == CACHE_SMALL:\n",
    "        pickle.dump(episodes[:100], f)\n",
    "    else:\n",
    "        pickle.dump(episodes, f)\n",
    "\n",
    "print(f\"Number of loaded episodes: {len(episodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-30T08:33:48.992799Z",
     "start_time": "2020-11-30T08:33:48.894182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>number</th>\n",
       "      <th>published</th>\n",
       "      <th>guests</th>\n",
       "      <th>main</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>views</th>\n",
       "      <th>commentCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6540Nct67uw</td>\n",
       "      <td>Joe Rogan Experience #1571 - Emily Harrington</td>\n",
       "      <td>1571.0</td>\n",
       "      <td>2020-11-27T18:00:01Z</td>\n",
       "      <td>Emily Harrington</td>\n",
       "      <td>True</td>\n",
       "      <td>14276</td>\n",
       "      <td>960</td>\n",
       "      <td>999045</td>\n",
       "      <td>3946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bvzs-VbEzds</td>\n",
       "      <td>Joe Rogan Experience #1570 - Willie D &amp; Mike J...</td>\n",
       "      <td>1570.0</td>\n",
       "      <td>2020-11-25T18:00:26Z</td>\n",
       "      <td>Willie D &amp; Mike Judge</td>\n",
       "      <td>True</td>\n",
       "      <td>18062</td>\n",
       "      <td>1351</td>\n",
       "      <td>1230183</td>\n",
       "      <td>6335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHXIQhvI8cw</td>\n",
       "      <td>Joe Rogan Experience #1569 - John Mackey</td>\n",
       "      <td>1569.0</td>\n",
       "      <td>2020-11-24T18:00:02Z</td>\n",
       "      <td>John Mackey</td>\n",
       "      <td>True</td>\n",
       "      <td>19961</td>\n",
       "      <td>4814</td>\n",
       "      <td>1492106</td>\n",
       "      <td>13619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2Wk6SL4ofG0</td>\n",
       "      <td>Joe Rogan Experience #1568 - Tom Green</td>\n",
       "      <td>1568.0</td>\n",
       "      <td>2020-11-20T18:00:01Z</td>\n",
       "      <td>Tom Green</td>\n",
       "      <td>True</td>\n",
       "      <td>32754</td>\n",
       "      <td>1798</td>\n",
       "      <td>2254148</td>\n",
       "      <td>14013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C7t_LxpzYTg</td>\n",
       "      <td>Joe Rogan Experience #1567 - Donnell Rawlings ...</td>\n",
       "      <td>1567.0</td>\n",
       "      <td>2020-11-19T18:00:01Z</td>\n",
       "      <td>Donnell Rawlings &amp; Dave Chappelle</td>\n",
       "      <td>True</td>\n",
       "      <td>139783</td>\n",
       "      <td>8702</td>\n",
       "      <td>6923358</td>\n",
       "      <td>42576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  number  \\\n",
       "0  6540Nct67uw      Joe Rogan Experience #1571 - Emily Harrington  1571.0   \n",
       "1  bvzs-VbEzds  Joe Rogan Experience #1570 - Willie D & Mike J...  1570.0   \n",
       "2  CHXIQhvI8cw           Joe Rogan Experience #1569 - John Mackey  1569.0   \n",
       "3  2Wk6SL4ofG0             Joe Rogan Experience #1568 - Tom Green  1568.0   \n",
       "4  C7t_LxpzYTg  Joe Rogan Experience #1567 - Donnell Rawlings ...  1567.0   \n",
       "\n",
       "              published                             guests  main   likes  \\\n",
       "0  2020-11-27T18:00:01Z                   Emily Harrington  True   14276   \n",
       "1  2020-11-25T18:00:26Z              Willie D & Mike Judge  True   18062   \n",
       "2  2020-11-24T18:00:02Z                        John Mackey  True   19961   \n",
       "3  2020-11-20T18:00:01Z                          Tom Green  True   32754   \n",
       "4  2020-11-19T18:00:01Z  Donnell Rawlings & Dave Chappelle  True  139783   \n",
       "\n",
       "  dislikes    views commentCount  \n",
       "0      960   999045         3946  \n",
       "1     1351  1230183         6335  \n",
       "2     4814  1492106        13619  \n",
       "3     1798  2254148        14013  \n",
       "4     8702  6923358        42576  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save CSV version of this to website directory\n",
    "ep_df = pd.DataFrame(\n",
    "    [\n",
    "        (e.video_id, e.title, e.number, e.published_at, e.guests, e.is_main_episode, e.likes, e.dislikes, e.views, e.comment_count) \n",
    "         for e in episodes\n",
    "    ],\n",
    "    columns=[\"id\", \"title\", \"number\", \"published\", \"guests\", \"main\", \"likes\", \"dislikes\",\"views\",\"commentCount\"]\n",
    ")\n",
    "\n",
    "ep_df.to_csv(WEBSITE + \"episodes.csv\")\n",
    "ep_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T19:39:31.302745Z",
     "start_time": "2020-11-27T19:39:24.438275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total main episodes 1470\n"
     ]
    }
   ],
   "source": [
    "# Kinda dumb but too lazy to fix this, go generate pickle file in 1.gen_episodes first\n",
    "CACHE = \"./jre-episodes.pickle\"\n",
    "\n",
    "with open(CACHE, \"rb\") as f:\n",
    "    episodes = pickle.load(f)\n",
    "\n",
    "main_eps = [e for e in episodes if e.is_main_episode]\n",
    "print(\"Total main episodes\", len(main_eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T20:30:34.646070Z",
     "start_time": "2020-11-27T20:22:31.764763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total missing main videos 11\n",
      "[youtube] wC5TVZ3p_H4: Downloading webpage\n",
      "[download] Destination: Joe Rogan Experience #1548 - Roy Jones Jr.-wC5TVZ3p_H4.mp4\n",
      "[download] 100% of 306.32MiB in 00:2467MiB/s ETA 00:005\n",
      "0\n",
      "[youtube] ckjwkCbGIu8: Downloading webpage\n",
      "[download] Destination: Joe Rogan Experience #1547 - Colin Quinn-ckjwkCbGIu8.mp4\n",
      "[download] 100% of 367.64MiB in 00:2737MiB/s ETA 00:002\n",
      "0\n",
      "[youtube] A9PfeA9qFp8: Downloading webpage\n",
      "[download] Destination: Joe Rogan Experience #1546 - Evan Hafer & Mat Best-A9PfeA9qFp8.mp4\n",
      "[download] 100% of 431.54MiB in 00:2803MiB/s ETA 00:002\n",
      "0\n",
      "[youtube] j-bSjzIPRro: Downloading webpage\n",
      "[download] Destination: Joe Rogan Experience #1545 - W. Keith Campbell-j-bSjzIPRro.mp4\n",
      "[download] 100% of 430.29MiB in 00:2899MiB/s ETA 00:001\n",
      "0\n",
      "[youtube] ikJq6wcgrXI: Downloading webpage\n",
      "[download] Destination: Joe Rogan Experience #1544 - Tim Dillon-ikJq6wcgrXI.mp4\n",
      "[download] 100% of 400.05MiB in 00:2442MiB/s ETA 00:001\n",
      "0\n",
      "[youtube] gzAQ7SklDxo: Downloading webpage\n",
      "[download] Destination: Joe Rogan Experience #1543 - Brian Muraresku & Graham Hancock-gzAQ7SklDxo.mp4\n",
      "[download] 100% of 447.81MiB in 00:2762MiB/s ETA 00:007\n",
      "0\n",
      "[youtube] EURQLMDNdw8: Downloading webpage\n",
      "[download] Destination: Joe Rogan Experience #1542 - Cameron Hanes-EURQLMDNdw8.mp4\n",
      "[download] 100% of 432.65MiB in 00:3319MiB/s ETA 00:008\n",
      "0\n",
      "[youtube] DYpotjw_KTw: Downloading webpage\n",
      "[download] Destination: Joe Rogan Experience #1541 - Bridget Phetasy-DYpotjw_KTw.mp4\n",
      "[download] 100% of 515.91MiB in 00:3288MiB/s ETA 00:007\n",
      "0\n",
      "[youtube] Kv3dsPkTvhY: Downloading webpage\n",
      "[download] Destination: Joe Rogan Experience #1540 - Frank von Hippel-Kv3dsPkTvhY.mp4\n",
      "[download] 100% of 410.34MiB in 00:2671MiB/s ETA 00:005\n",
      "0\n",
      "[youtube] LNw8cGy7R_s: Downloading webpage\n",
      "[youtube] LNw8cGy7R_s: Downloading embed webpage\n",
      "[youtube] LNw8cGy7R_s: Refetching age-gated info webpage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: This video is not available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not download Joe Rogan Experience #674 - Brian Redban\n",
      "[youtube] 0swiKKUHIiU: Downloading webpage\n",
      "[youtube] 0swiKKUHIiU: Downloading embed webpage\n",
      "[youtube] 0swiKKUHIiU: Refetching age-gated info webpage\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: This video contains content from NFL, who has blocked it on copyright grounds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "could not download Joe Rogan Experience #572 - Dom Irrera\n",
      "Downloaded 9 videos\n"
     ]
    }
   ],
   "source": [
    "video_files = list(glob(VIDEOS_LOCATION + \"*.mp4\"))\n",
    "video_files_ids = [v[-15:-4] for v in video_files]\n",
    "downloaded_main = [e for e in main_eps if e.video_id in video_files_ids]\n",
    "\n",
    "# sort to give priority to newer episodes first\n",
    "missing_videos = sorted(\n",
    "    list(set(main_eps) - set(downloaded_main)), \n",
    "    key=lambda ep: ep.number, \n",
    "    reverse=True,\n",
    ")\n",
    "\n",
    "print(\"Total missing main videos\", len(missing_videos))\n",
    "\n",
    "ydl = youtube_dl.YoutubeDL(\n",
    "    {\n",
    "        # MP4 at 360p\n",
    "        \"format\": \"18\",\n",
    "        # \"cookiefile\": \"./youtube-dl-cookies.txt\"\n",
    "    }\n",
    ")\n",
    "\n",
    "downloaded = 0\n",
    "with ydl:\n",
    "    for ep in missing_videos:\n",
    "        video = f\"http://www.youtube.com/watch?v={ep.video_id}\"\n",
    "\n",
    "        try:\n",
    "            result = ydl.download([video])\n",
    "            print(result)\n",
    "            downloaded += 1\n",
    "            \n",
    "            # Couldn't get option \"outtml\" to work, manually move instead\n",
    "            try:\n",
    "                file = f\"{ep.title}-{ep.video_id}.mp4\"\n",
    "                shutil.move(f\"./{file}\", VIDEOS_LOCATION + file)\n",
    "            except Exception as e:\n",
    "                print(\"Could not move\", ep, e)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(\"could not download\", ep.title)\n",
    "        \n",
    "        time.sleep(15)\n",
    "\n",
    "print(\"Downloaded\", downloaded, \"videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unused but may be helpful to someone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(videoId, max=1500):\n",
    "    page_token = None\n",
    "    items = []\n",
    "    # print(videoId)\n",
    "\n",
    "    while True and len(items) < max:\n",
    "        # print(f\"\\t{len(items)}/{max}\")\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=\"id,snippet\",\n",
    "            videoId=videoId,\n",
    "            pageToken=page_token,\n",
    "            maxResults=500,\n",
    "            order=\"relevance\",\n",
    "        )\n",
    "        response = request.execute()\n",
    "        items.extend(response[\"items\"])\n",
    "\n",
    "        if \"nextPageToken\" in response:\n",
    "            page_token = response[\"nextPageToken\"]\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return items\n",
    "\n",
    "def get_comments():\n",
    "#     Get comments and statistics for each video\n",
    "    with open(UPLOADS, \"r\") as f:\n",
    "        uploads = json.loads(f.read())\n",
    "\n",
    "        # likeCount, textDisplay, textOriginal, publishedAt\n",
    "        # authorDisplayName, authorChannelId\n",
    "        i = 0\n",
    "        comment_threads = {}\n",
    "        with open(COMMENTS, \"r\") as f:\n",
    "            comment_threads = json.load(f)\n",
    "\n",
    "        for upload in tqdm.tqdm(uploads):\n",
    "            videoId = upload[\"snippet\"][\"resourceId\"][\"videoId\"]\n",
    "\n",
    "            if videoId not in comment_threads:\n",
    "                comments = get_comments(videoId)\n",
    "                comment_threads[videoId] = comments\n",
    "                print(videoId, len(comments))\n",
    "            else:\n",
    "                print(\"Already saved thread for\", videoId)\n",
    "\n",
    "            i += 1\n",
    "            # save progress\n",
    "            if i % 25 == 0:\n",
    "                with open(COMMENTS, \"w\") as f:\n",
    "                    f.write(json.dumps(comment_threads))\n",
    "\n",
    "        with open(COMMENTS, \"w\") as f:\n",
    "            f.write(json.dumps(comment_threads))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use video data instead\n",
    "def download_audio():\n",
    "    audio_file = lambda id: f\"data/jre/audio/{id}\"\n",
    "    with open(UPLOADS, \"r\") as f:\n",
    "        uploads = json.loads(f.read())\n",
    "\n",
    "        for upload in tqdm(uploads):\n",
    "            id = upload[\"contentDetails\"][\"videoId\"]\n",
    "            yt = None\n",
    "            if os.path.exists(audio_file(id)):\n",
    "                print(\"File exists\", id, \"skipping...\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                yt = YouTube(f\"https://www.youtube.com/watch?v={id}\")\n",
    "            except Exception as e:\n",
    "                print(\"Could not load api\", id, str(e))\n",
    "                continue\n",
    "\n",
    "            audio_streams = yt.streams.filter(only_audio=True)\n",
    "\n",
    "            if len(audio_streams) == 0:\n",
    "                print(\"No audio streams available for\", id)\n",
    "                continue\n",
    "\n",
    "            mp4_streams = [s for s in audio_streams if s.mime_type == \"audio/mp4\"]\n",
    "            stream = audio_streams[0] if len(mp4_streams) == 0 else mp4_streams[0]\n",
    "\n",
    "            print(f\"Downloading [{id}] {stream.mime_type} {stream.abr}\")\n",
    "            stream.download(audio_file(id))\n",
    "            print(\"\\t... Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unused utils\n",
    "def get_timestamps(c):\n",
    "    timestamp = r\"(\\d?\\d?:)?\\d?\\d?:\\d{2}\"\n",
    "\n",
    "    if \"topLevelComment\" in c[\"snippet\"]:\n",
    "        snippet = c[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "\n",
    "        if re.search(timestamp, snippet[\"textOriginal\"]) is None:\n",
    "            return None\n",
    "        return snippet[\"likeCount\"], snippet[\"textOriginal\"]\n",
    "    elif re.search(timestamp, c[\"snippet\"][\"textOriginal\"]) is not None:\n",
    "        return c[\"snippet\"][\"likeCount\"], c[\"snippet\"][\"textOriginal\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
