{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data collection \n",
    "1. TFIDF For all episodes\n",
    "2. Cosine similarity\n",
    "3. Most spoken words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T19:37:52.338929Z",
     "start_time": "2020-11-29T19:37:52.324061Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-30T00:38:46.951058Z",
     "start_time": "2020-11-30T00:38:46.512673Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-30T00:38:48.287204Z",
     "start_time": "2020-11-30T00:38:48.257785Z"
    }
   },
   "outputs": [],
   "source": [
    "WEBSITE = \"../../jre-vis/public/\"\n",
    "N_TOP_WORD_OCCURRENCES = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T19:37:58.704873Z",
     "start_time": "2020-11-29T19:37:52.845375Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded episodes: 2462\n"
     ]
    }
   ],
   "source": [
    "CACHE = \"./jre-episodes.pickle\"\n",
    "\n",
    "with open(CACHE, \"rb\") as f:\n",
    "    episodes = pickle.load(f)\n",
    "\n",
    "print(f\"Number of loaded episodes: {len(episodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-30T00:43:45.643793Z",
     "start_time": "2020-11-30T00:38:53.770663Z"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2462/2462 [04:51<00:00,  8.44it/s] \n"
     ]
    }
   ],
   "source": [
    "from lib.TFIDF import TFIDF\n",
    "from lib.utils import clean_text\n",
    "\n",
    "cleaned_corpus = [(e, clean_text(e.text)) for e in tqdm(episodes) if e.captions is not None and e.is_main_episode]\n",
    "# s2w is the stem 2 word dictionary (saved in a later cell)\n",
    "corpus = [(ep, cleaned) for ep, (cleaned, s2w) in cleaned_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-30T02:32:14.068440Z",
     "start_time": "2020-11-30T02:32:14.018061Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8626908"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Words analyzed\n",
    "np.sum([len(words) for e, words in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T19:37:58.870115Z",
     "start_time": "2020-11-29T19:37:58.709276Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TFIDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-98e35560ef54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFIDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tfidf-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCACHE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TFIDF' is not defined"
     ]
    }
   ],
   "source": [
    "tfidf = TFIDF()\n",
    "tfidf.generate(corpus)\n",
    "\n",
    "with open(\"tfidf-\" + CACHE[2:], \"wb\") as f:\n",
    "    pickle.dump(tfidf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T19:38:15.666553Z",
     "start_time": "2020-11-29T19:38:02.271089Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"tfidf-\" + CACHE[2:], \"rb\") as f:\n",
    "    tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T19:38:16.402484Z",
     "start_time": "2020-11-29T19:38:15.668899Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C7t_LxpzYTg] Joe Rogan Experience #1567 - Donnell Rawlings & Dave Chappelle\n",
      "soy 0.02732\n",
      "biden 0.02731\n",
      "drone 0.02472\n",
      "vaccin 0.02416\n",
      "yo 0.02319\n",
      "dog 0.02119\n",
      "trump 0.02069\n",
      "edamam 0.01957\n",
      "cook 0.01912\n",
      "elk 0.01854\n",
      "\n",
      "[Sc5oa6MqPDg] Joe Rogan Experience #1566 - Nicholas Christakis\n",
      "vaccin 0.10879\n",
      "viru 0.10291\n",
      "pandem 0.06037\n",
      "infect 0.04149\n",
      "immun 0.04109\n",
      "mask 0.03536\n",
      "dexamethason 0.03439\n",
      "coronaviru 0.02873\n",
      "incident 0.02562\n",
      "antibodi 0.0239\n",
      "\n",
      "[GmHwG2p_esE] Joe Rogan Experience #1565 - Gary Laderman\n",
      "religion 0.05508\n",
      "religi 0.03545\n",
      "psychedel 0.03242\n",
      "lsd 0.02355\n",
      "topic 0.02197\n",
      "sacr 0.02021\n",
      "drug 0.0191\n",
      "sexual 0.0156\n",
      "pornographi 0.01486\n",
      "student 0.01453\n",
      "\n",
      "[5PrLGhJnO7I] Joe Rogan Experience #1562 - Dave Smith\n",
      "biden 0.05802\n",
      "trump 0.04451\n",
      "donald 0.0245\n",
      "war 0.02359\n",
      "eisenhow 0.01875\n",
      "iraq 0.0172\n",
      "kamala 0.01718\n",
      "berni 0.01522\n",
      "sander 0.01475\n",
      "presid 0.01453\n",
      "\n",
      "[C8M1ZRYt-2Q] Joe Rogan Experience #1561 - Kermit Pattison\n",
      "skeleton 0.11084\n",
      "arti 0.07028\n",
      "fossil 0.06005\n",
      "speci 0.0549\n",
      "canin 0.05237\n",
      "luci 0.0466\n",
      "ethiopia 0.04224\n",
      "ancestor 0.03509\n",
      "neanderth 0.03356\n",
      "geolog 0.03043\n",
      "\n",
      "[surnFz_pZE4] Joe Rogan Experience #1559 - Steven Rinella\n",
      "mammoth 0.01886\n",
      "biden 0.01729\n",
      "d-day 0.01474\n",
      "wolv 0.01439\n",
      "hunt 0.01362\n",
      "baculum 0.01355\n",
      "fossil 0.01318\n",
      "overproduc 0.01263\n",
      "bear 0.0122\n",
      "bone 0.01209\n",
      "\n",
      "[KkjxSKrcbOg] JRE End Of The World #2\n",
      "biden 0.08165\n",
      "trump 0.06636\n",
      "pennsylvania 0.06632\n",
      "vote 0.04905\n",
      "poll 0.03979\n",
      "mail-in 0.02749\n",
      "win 0.02542\n",
      "florida 0.02356\n",
      "elect 0.02317\n",
      "ohio 0.02003\n",
      "\n",
      "[OaTKaHKCAFg] Joe Rogan Experience #1558 - Tristan Harris\n",
      "facebook 0.04669\n",
      "attent 0.02275\n",
      "social 0.02149\n",
      "algorithm 0.01921\n",
      "voodoo 0.01895\n",
      "huxley 0.01874\n",
      "orwel 0.0171\n",
      "dilemma 0.01535\n",
      "media 0.01533\n",
      "ukulel 0.01516\n",
      "\n",
      "[4ugp6FBq6E0] Joe Rogan Experience #1557 - Gad Saad\n",
      "intellectu 0.01844\n",
      "victimolog 0.01642\n",
      "regret 0.01606\n",
      "jillian 0.01421\n",
      "publish 0.01402\n",
      "academ 0.01371\n",
      "biden 0.0131\n",
      "lockdown 0.01286\n",
      "narrat 0.01275\n",
      "parasit 0.01232\n",
      "\n",
      "[t0rcLsoIKgA] Joe Rogan Experience #1556 - Glenn Greenwald\n",
      "snowden 0.03728\n",
      "biden 0.03153\n",
      "journalist 0.02731\n",
      "tran 0.01996\n",
      "martina 0.01876\n",
      "govern 0.01663\n",
      "brazil 0.01582\n",
      "trump 0.01475\n",
      "censorship 0.01456\n",
      "aclu 0.01328\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf.print_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T19:24:05.365155Z",
     "start_time": "2020-11-29T19:24:05.319283Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    dist = lambda tfidf: np.sqrt(np.sum(tfidf * tfidf))\n",
    "    dot_prod = np.dot(a, b)\n",
    "    distances = dist(a) * dist(b)\n",
    "    return dot_prod / distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T19:24:05.400430Z",
     "start_time": "2020-11-29T19:24:05.366775Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cos_sim_matrix():\n",
    "    # Probably the dumbest way to do this\n",
    "    tfidf_titles = [e.title for e in tfidf.scores.keys()]\n",
    "    index_of_ep = lambda ep: tfidf_titles.index(ep.title)\n",
    "\n",
    "    # (ep1, ep2), score\n",
    "    cos_sim_matrix = np.zeros((len(episodes), len(episodes)), tuple)\n",
    "\n",
    "    for a, b in tqdm([(a, b) for a in tfidf.scores for b in tfidf.scores]):\n",
    "        ai = index_of_ep(a)\n",
    "        bi = index_of_ep(b)\n",
    "        # Only fill half of the matrix\n",
    "        if bi > ai:\n",
    "            continue\n",
    "        cos_sim_matrix[ai][bi] = ((a, b), cosine_similarity(tfidf.scores[a], tfidf.scores[b]))\n",
    "        \n",
    "    return cos_sim_matrix\n",
    "\n",
    "def convert_matrix_to_list(cos_sim_matrix):\n",
    "    cos_sim_list = []\n",
    "\n",
    "    # Format as (index, index), similarity\n",
    "    for row in cos_sim_matrix:\n",
    "        for item in row:\n",
    "            if item == 0: continue\n",
    "            (a, b), score = item\n",
    "            if a == b: continue\n",
    "            cos_sim_list.append((a, b, score))\n",
    "            \n",
    "    return cos_sim_list\n",
    "\n",
    "def print_most_similar(cos_sim_list):\n",
    "    print(\"Most similar podcast episodes\")\n",
    "    print(\"=============================\\n\")\n",
    "    cos_sim_list = sorted(cos_sim_list, key=lambda x: x[2], reverse=True)\n",
    "    for a, b, score in cos_sim_list[:50]:\n",
    "        if a.is_main_episode and b.is_main_episode:\n",
    "            print(a)\n",
    "            print(b)\n",
    "            print(f\"\\t{round(score, 4) * 100}%\")\n",
    "            print()\n",
    "            \n",
    "def to_cos_sim_table(cos_sim_list):\n",
    "    return pd.DataFrame(\n",
    "        [(a.video_id, b.video_id, s) for a,b,s in cos_sim_list], \n",
    "        columns=[\"id1\", \"id2\", \"similarity\"],\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T19:38:16.563092Z",
     "start_time": "2020-11-29T19:38:16.406778Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOAD_COS_SIM = True\n",
    "\n",
    "if LOAD_COS_SIM:\n",
    "    with open(\"cos-sim-\" + CACHE[2:], \"rb\") as f:\n",
    "        cos_sim_table = pickle.load(f)\n",
    "else:\n",
    "    cos_sim_matrix = get_cos_sim_matrix()\n",
    "    \n",
    "    print(cos_sim_matrix[:3], len(cos_sim_matrix))\n",
    "    \n",
    "    cos_sim_list = convert_matrix_to_list(cos_sim_matrix)\n",
    "    \n",
    "    print_most_similar(cos_sim_list)\n",
    "    \n",
    "    cos_sim_table = to_cos_sim_table(cos_sim_list)\n",
    "    \n",
    "    with open(\"cos-sim-\" + CACHE[2:], \"wb\") as f:\n",
    "        pickle.dump(cos_sim_table, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To protobufs\n",
    "63mb -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T19:47:43.597411Z",
     "start_time": "2020-11-27T19:47:43.484358Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# protoc --python_out=./ ./episode-sim.proto\n",
    "# pbf ./episode-sim.proto --browser > ../../jre-vis/public/\n",
    "import episode_sim_pb2 as ep_proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T23:51:28.085688Z",
     "start_time": "2020-11-27T19:47:43.599973Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1292028it [4:03:44, 88.35it/s] \n"
     ]
    }
   ],
   "source": [
    "ids = cos_sim_table[\"id1\"].append(cos_sim_table[\"id2\"]).unique()\n",
    "IDs = ep_proto.IDs()\n",
    "for i, id in enumerate(ids):\n",
    "    row = IDs.rows.add()\n",
    "    row.idNum = i\n",
    "    row.id = id\n",
    "    \n",
    "epSims = ep_proto.EpisodeSims()\n",
    "for index, row in tqdm(cos_sim_table.iterrows()):\n",
    "    e = epSims.rows.add()\n",
    "    e.similarity = row[\"similarity\"]\n",
    "    e.idNum1 = [i for i, id in enumerate(ids) if id == row[\"id1\"]][0]\n",
    "    e.idNum2 = [i for i, id in enumerate(ids) if id == row[\"id2\"]][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-27T23:51:30.071636Z",
     "start_time": "2020-11-27T23:51:28.102177Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(WEBSITE + \"ep_sim\", \"wb\") as f:\n",
    "    f.write(epSims.SerializeToString())\n",
    "    \n",
    "with open(WEBSITE + \"ep_sim_id_lookup\", \"wb\") as f:\n",
    "    f.write(IDs.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store top word occurrences of each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T19:38:17.827864Z",
     "start_time": "2020-11-29T19:38:16.565001Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_num(e):\n",
    "    # TODO\n",
    "    # unsure why, but the tfidf ep #1564 and #1563 in the cfd\n",
    "    # are strings and not actual episodes\n",
    "    try:\n",
    "        x = e[0].number if e[0].number is not None else -1\n",
    "        return x\n",
    "    except Exception as x:\n",
    "        return -1\n",
    "    \n",
    "cfd_items = sorted(list(tfidf.cfd.items()), key=get_num, reverse=True)\n",
    "most_common = [(k.video_id, dict(v.most_common(N_TOP_WORD_OCCURRENCES))) for k, v in cfd_items]\n",
    "cfd_table = pd.DataFrame(\n",
    "    most_common,\n",
    "    columns=[\"id\", \"top_words\"],\n",
    ")\n",
    "\n",
    "cfd_table.to_csv(WEBSITE + \"word_occurrences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T19:38:19.191974Z",
     "start_time": "2020-11-29T19:38:19.009060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19921"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "most_common_words = set(flatten([list(w.keys()) for v, w in most_common]))\n",
    "most_common_words = [(i, w) for i,w in enumerate(most_common_words)]\n",
    "len(most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T19:38:20.805009Z",
     "start_time": "2020-11-29T19:38:20.733444Z"
    }
   },
   "outputs": [],
   "source": [
    "# protoc --python_out=./ ./most-common.proto\n",
    "# pbf ./most-common.proto --browser > ../../jre-vis/src/lib/proto/most-common.js\n",
    "import most_common_pb2 as mc_proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T21:23:28.844946Z",
     "start_time": "2020-11-29T21:23:28.771030Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0/1608 0.0 seconds per, est: 0.0mins\r",
      "C7t_LxpzYTg\n"
     ]
    }
   ],
   "source": [
    "# Attempt at word_occurrences minification, only shaves ~2mb and takes about 10seconds to parse client-side\n",
    "from time import time \n",
    "\n",
    "wct = mc_proto.WordLookupTable()\n",
    "for i, word in most_common_words:\n",
    "    r = wct.rows.add()\n",
    "    r.word = word\n",
    "    r.id = i\n",
    "    \n",
    "mct = mc_proto.MostCommonTable()\n",
    "l = time()\n",
    "\n",
    "print()\n",
    "for i, X in cfd_table.iterrows():\n",
    "    s = time()\n",
    "    taken = round((s - l), 2)\n",
    "    est = round(taken * len(cfd_table)/60, 2)\n",
    "    l = s\n",
    "    print(f\"{i}/{len(cfd_table)}\", f\"{taken} seconds per, est: {est}mins\", end=\"\\r\")\n",
    "\n",
    "    r = mct.rows.add()\n",
    "    r.id = X['id']\n",
    "\n",
    "    for word, count in X['top_words'].items():        \n",
    "        w = r.words.add()\n",
    "        w.id = [i for i,w in most_common_words if w == word][0]\n",
    "        w.occurrences = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T21:20:35.621169Z",
     "start_time": "2020-11-29T21:20:32.996738Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(WEBSITE + \"word_occurrences\", \"wb\") as f:\n",
    "    f.write(mct.SerializeToString())\n",
    "    \n",
    "with open(WEBSITE + \"word_occurrences_id_lookup\", \"wb\") as f:\n",
    "    f.write(wct.SerializeToString())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store top TFIDF of each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T00:18:11.301597Z",
     "start_time": "2020-11-28T00:16:31.715127Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_table = pd.DataFrame(\n",
    "    [(e.video_id, [w for w,s in tfidf.get_scores(e)[:10]]) for e, _ in tfidf.scores.items()],\n",
    "    columns=[\"id\", \"top_words\"],\n",
    ")\n",
    "\n",
    "tfidf_table.to_csv(WEBSITE + \"top_tfidf.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Stem Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-28T00:06:06.512546Z",
     "start_time": "2020-11-28T00:06:03.009318Z"
    }
   },
   "outputs": [],
   "source": [
    "reverse_stem = {}\n",
    "for title, (cleaned, s2w) in cleaned_corpus:\n",
    "    reverse_stem.update(s2w)\n",
    "    \n",
    "rm_stem = lambda stem, w: w if len(stem) == len(w) else w[len(stem):]\n",
    "reverse_stem = {stem: [rm_stem(stem, w) for w in words] for stem, words in reverse_stem.items()}\n",
    "\n",
    "# Remove items with 1 element that is the exact same as the stem\n",
    "reverse_stem = {stem: words for stem, words in reverse_stem.items() if len(words) != 1 or words[0] != stem}\n",
    "\n",
    "# Remove words that are the exact same as the stem\n",
    "reverse_stem = {stem: [w for w in words if w != stem] for stem, words in reverse_stem.items()}\n",
    "    \n",
    "with open(WEBSITE + \"reverse_stem.json\", \"w\") as f:\n",
    "    f.write(json.dumps(reverse_stem))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
